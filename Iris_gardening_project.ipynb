{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi_class Classification Iris Flowers Project :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# objective:Botanist wants to determine the species of an iris flower based on characteristics of that flower.For instance attributes including petal,petal width,sepal length,sepal width are \"features\" that determine the classification of a given iris flower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start off by importing all of the classes and functions we will need. This includes both the functionality we require from Keras, but also data loading from pandas as well as data preparation and model evaluation from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the iris dataset  into scikit_learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import load_iris function from dataset module\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save bunch object containing iris_dataset and iris attributes\n",
    "data_frame_iris  = load_iris()\n",
    "type(data_frame_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n"
     ]
    }
   ],
   "source": [
    "# print the iris data\n",
    "print(data_frame_iris.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning Termonolgy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " •\tEach row is an observation is (also known as: samples, instance   ).\n",
    " \n",
    " •\tEach column is a feature (also know as : predicator, attributes, input ,covariate).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "# print the names of features\n",
    "print(data_frame_iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "# print the integers the species of each observation\n",
    "print(data_frame_iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# print encoding scheme for sepecies: 0 = \"setosa\", 1 = \"versicolor\", 2 = \"virginica\"\n",
    "print(data_frame_iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " •\tEach  value we are predicting is the response (also known as: target ,outcomes,label,dependent variable).\n",
    " \n",
    " •\tClassification is supervised learning in which the response is categorical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# check the type of featurs and response\n",
    "print(type(data_frame_iris.data))\n",
    "print(type(data_frame_iris.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "# check the shape of features and response\n",
    "print(data_frame_iris.data.shape)\n",
    "print(data_frame_iris.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n"
     ]
    }
   ],
   "source": [
    "# store feature matrix in x\n",
    "x =data_frame_iris[\"data\"]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# store response matrix in y\n",
    "y =to_categorical(data_frame_iris[\"target\"])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data  and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.7 3.  4.2 1.2]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [5.  3.  1.6 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.  5.  1.7]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [6.2 2.2 4.5 1.5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_train)\n",
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.8 1.5 0.3]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  2.  3.5 1. ]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [4.4 3.  1.3 0.2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_test)\n",
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train)\n",
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_test)\n",
    "len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0712 22:00:52.417979 11288 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0712 22:00:52.519981 11288 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0712 22:00:52.532673 11288 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(10,input_dim = 4 ,activation = \"relu\"))\n",
    "model.add(layers.Dense(3,activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0712 22:00:52.658963 11288 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0712 22:00:52.719773 11288 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0712 22:00:53.190862 11288 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0712 22:00:53.322661 11288 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90 samples, validate on 60 samples\n",
      "Epoch 1/200\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.2633 - acc: 0.3889 - val_loss: 1.4064 - val_acc: 0.2500\n",
      "Epoch 2/200\n",
      "90/90 [==============================] - 0s 400us/step - loss: 1.2113 - acc: 0.3889 - val_loss: 1.3346 - val_acc: 0.2500\n",
      "Epoch 3/200\n",
      "90/90 [==============================] - 0s 547us/step - loss: 1.1536 - acc: 0.3889 - val_loss: 1.2708 - val_acc: 0.2500\n",
      "Epoch 4/200\n",
      "90/90 [==============================] - 0s 576us/step - loss: 1.1032 - acc: 0.3889 - val_loss: 1.2074 - val_acc: 0.2500\n",
      "Epoch 5/200\n",
      "90/90 [==============================] - 0s 654us/step - loss: 1.0560 - acc: 0.3889 - val_loss: 1.1460 - val_acc: 0.2500\n",
      "Epoch 6/200\n",
      "90/90 [==============================] - 0s 945us/step - loss: 1.0055 - acc: 0.3889 - val_loss: 1.0913 - val_acc: 0.2667\n",
      "Epoch 7/200\n",
      "90/90 [==============================] - 0s 587us/step - loss: 0.9659 - acc: 0.4444 - val_loss: 1.0397 - val_acc: 0.3333\n",
      "Epoch 8/200\n",
      "90/90 [==============================] - 0s 471us/step - loss: 0.9272 - acc: 0.5111 - val_loss: 0.9949 - val_acc: 0.4333\n",
      "Epoch 9/200\n",
      "90/90 [==============================] - 0s 509us/step - loss: 0.8915 - acc: 0.5556 - val_loss: 0.9550 - val_acc: 0.5500\n",
      "Epoch 10/200\n",
      "90/90 [==============================] - 0s 593us/step - loss: 0.8566 - acc: 0.6333 - val_loss: 0.9172 - val_acc: 0.5833\n",
      "Epoch 11/200\n",
      "90/90 [==============================] - 0s 389us/step - loss: 0.8225 - acc: 0.6333 - val_loss: 0.8895 - val_acc: 0.5833\n",
      "Epoch 12/200\n",
      "90/90 [==============================] - 0s 431us/step - loss: 0.7985 - acc: 0.6222 - val_loss: 0.8711 - val_acc: 0.5333\n",
      "Epoch 13/200\n",
      "90/90 [==============================] - 0s 410us/step - loss: 0.7797 - acc: 0.6222 - val_loss: 0.8479 - val_acc: 0.6000\n",
      "Epoch 14/200\n",
      "90/90 [==============================] - 0s 570us/step - loss: 0.7616 - acc: 0.6333 - val_loss: 0.8263 - val_acc: 0.6167\n",
      "Epoch 15/200\n",
      "90/90 [==============================] - 0s 511us/step - loss: 0.7457 - acc: 0.6333 - val_loss: 0.8117 - val_acc: 0.6167\n",
      "Epoch 16/200\n",
      "90/90 [==============================] - 0s 358us/step - loss: 0.7308 - acc: 0.6444 - val_loss: 0.7940 - val_acc: 0.6000\n",
      "Epoch 17/200\n",
      "90/90 [==============================] - 0s 499us/step - loss: 0.7167 - acc: 0.6667 - val_loss: 0.7800 - val_acc: 0.6333\n",
      "Epoch 18/200\n",
      "90/90 [==============================] - 0s 457us/step - loss: 0.7033 - acc: 0.6778 - val_loss: 0.7664 - val_acc: 0.6333\n",
      "Epoch 19/200\n",
      "90/90 [==============================] - 0s 388us/step - loss: 0.6912 - acc: 0.6889 - val_loss: 0.7552 - val_acc: 0.6333\n",
      "Epoch 20/200\n",
      "90/90 [==============================] - 0s 432us/step - loss: 0.6792 - acc: 0.6889 - val_loss: 0.7449 - val_acc: 0.6500\n",
      "Epoch 21/200\n",
      "90/90 [==============================] - 0s 433us/step - loss: 0.6681 - acc: 0.7000 - val_loss: 0.7348 - val_acc: 0.6667\n",
      "Epoch 22/200\n",
      "90/90 [==============================] - 0s 400us/step - loss: 0.6569 - acc: 0.7000 - val_loss: 0.7238 - val_acc: 0.6667\n",
      "Epoch 23/200\n",
      "90/90 [==============================] - 0s 459us/step - loss: 0.6462 - acc: 0.7111 - val_loss: 0.7143 - val_acc: 0.6833\n",
      "Epoch 24/200\n",
      "90/90 [==============================] - 0s 434us/step - loss: 0.6360 - acc: 0.7111 - val_loss: 0.7046 - val_acc: 0.6833\n",
      "Epoch 25/200\n",
      "90/90 [==============================] - 0s 438us/step - loss: 0.6268 - acc: 0.7111 - val_loss: 0.6950 - val_acc: 0.6667\n",
      "Epoch 26/200\n",
      "90/90 [==============================] - 0s 366us/step - loss: 0.6175 - acc: 0.7222 - val_loss: 0.6872 - val_acc: 0.7167\n",
      "Epoch 27/200\n",
      "90/90 [==============================] - 0s 477us/step - loss: 0.6085 - acc: 0.7333 - val_loss: 0.6774 - val_acc: 0.7000\n",
      "Epoch 28/200\n",
      "90/90 [==============================] - 0s 389us/step - loss: 0.6003 - acc: 0.7333 - val_loss: 0.6688 - val_acc: 0.6833\n",
      "Epoch 29/200\n",
      "90/90 [==============================] - 0s 421us/step - loss: 0.5915 - acc: 0.7556 - val_loss: 0.6619 - val_acc: 0.7667\n",
      "Epoch 30/200\n",
      "90/90 [==============================] - 0s 270us/step - loss: 0.5835 - acc: 0.7667 - val_loss: 0.6558 - val_acc: 0.7667\n",
      "Epoch 31/200\n",
      "90/90 [==============================] - 0s 255us/step - loss: 0.5776 - acc: 0.7556 - val_loss: 0.6471 - val_acc: 0.7167\n",
      "Epoch 32/200\n",
      "90/90 [==============================] - 0s 421us/step - loss: 0.5686 - acc: 0.7667 - val_loss: 0.6419 - val_acc: 0.7833\n",
      "Epoch 33/200\n",
      "90/90 [==============================] - 0s 327us/step - loss: 0.5611 - acc: 0.7889 - val_loss: 0.6343 - val_acc: 0.7833\n",
      "Epoch 34/200\n",
      "90/90 [==============================] - 0s 355us/step - loss: 0.5545 - acc: 0.8000 - val_loss: 0.6275 - val_acc: 0.7833\n",
      "Epoch 35/200\n",
      "90/90 [==============================] - 0s 299us/step - loss: 0.5483 - acc: 0.7889 - val_loss: 0.6222 - val_acc: 0.7833\n",
      "Epoch 36/200\n",
      "90/90 [==============================] - 0s 270us/step - loss: 0.5419 - acc: 0.8000 - val_loss: 0.6151 - val_acc: 0.8000\n",
      "Epoch 37/200\n",
      "90/90 [==============================] - 0s 399us/step - loss: 0.5347 - acc: 0.8000 - val_loss: 0.6095 - val_acc: 0.8000\n",
      "Epoch 38/200\n",
      "90/90 [==============================] - 0s 378us/step - loss: 0.5305 - acc: 0.8444 - val_loss: 0.6053 - val_acc: 0.8500\n",
      "Epoch 39/200\n",
      "90/90 [==============================] - 0s 366us/step - loss: 0.5236 - acc: 0.8222 - val_loss: 0.5984 - val_acc: 0.8000\n",
      "Epoch 40/200\n",
      "90/90 [==============================] - 0s 255us/step - loss: 0.5172 - acc: 0.8000 - val_loss: 0.5932 - val_acc: 0.8000\n",
      "Epoch 41/200\n",
      "90/90 [==============================] - 0s 366us/step - loss: 0.5124 - acc: 0.8000 - val_loss: 0.5875 - val_acc: 0.8000\n",
      "Epoch 42/200\n",
      "90/90 [==============================] - 0s 378us/step - loss: 0.5062 - acc: 0.8111 - val_loss: 0.5830 - val_acc: 0.8167\n",
      "Epoch 43/200\n",
      "90/90 [==============================] - 0s 377us/step - loss: 0.5020 - acc: 0.8556 - val_loss: 0.5791 - val_acc: 0.8500\n",
      "Epoch 44/200\n",
      "90/90 [==============================] - 0s 288us/step - loss: 0.4966 - acc: 0.9111 - val_loss: 0.5736 - val_acc: 0.8333\n",
      "Epoch 45/200\n",
      "90/90 [==============================] - 0s 252us/step - loss: 0.4912 - acc: 0.8889 - val_loss: 0.5689 - val_acc: 0.8333\n",
      "Epoch 46/200\n",
      "90/90 [==============================] - 0s 332us/step - loss: 0.4866 - acc: 0.8889 - val_loss: 0.5638 - val_acc: 0.8333\n",
      "Epoch 47/200\n",
      "90/90 [==============================] - 0s 288us/step - loss: 0.4831 - acc: 0.9111 - val_loss: 0.5599 - val_acc: 0.8500\n",
      "Epoch 48/200\n",
      "90/90 [==============================] - 0s 293us/step - loss: 0.4774 - acc: 0.9333 - val_loss: 0.5558 - val_acc: 0.8500\n",
      "Epoch 49/200\n",
      "90/90 [==============================] - 0s 266us/step - loss: 0.4730 - acc: 0.9222 - val_loss: 0.5505 - val_acc: 0.8500\n",
      "Epoch 50/200\n",
      "90/90 [==============================] - 0s 233us/step - loss: 0.4686 - acc: 0.9000 - val_loss: 0.5462 - val_acc: 0.8333\n",
      "Epoch 51/200\n",
      "90/90 [==============================] - 0s 406us/step - loss: 0.4643 - acc: 0.9111 - val_loss: 0.5426 - val_acc: 0.8500\n",
      "Epoch 52/200\n",
      "90/90 [==============================] - 0s 288us/step - loss: 0.4597 - acc: 0.9444 - val_loss: 0.5387 - val_acc: 0.8500\n",
      "Epoch 53/200\n",
      "90/90 [==============================] - 0s 288us/step - loss: 0.4555 - acc: 0.9444 - val_loss: 0.5350 - val_acc: 0.8500\n",
      "Epoch 54/200\n",
      "90/90 [==============================] - 0s 222us/step - loss: 0.4514 - acc: 0.9444 - val_loss: 0.5314 - val_acc: 0.8500\n",
      "Epoch 55/200\n",
      "90/90 [==============================] - 0s 245us/step - loss: 0.4480 - acc: 0.9333 - val_loss: 0.5269 - val_acc: 0.8500\n",
      "Epoch 56/200\n",
      "90/90 [==============================] - 0s 265us/step - loss: 0.4436 - acc: 0.9333 - val_loss: 0.5233 - val_acc: 0.8500\n",
      "Epoch 57/200\n",
      "90/90 [==============================] - 0s 289us/step - loss: 0.4397 - acc: 0.9444 - val_loss: 0.5193 - val_acc: 0.8667\n",
      "Epoch 58/200\n",
      "90/90 [==============================] - 0s 347us/step - loss: 0.4361 - acc: 0.9556 - val_loss: 0.5163 - val_acc: 0.8833\n",
      "Epoch 59/200\n",
      "90/90 [==============================] - 0s 277us/step - loss: 0.4321 - acc: 0.9556 - val_loss: 0.5124 - val_acc: 0.8833\n",
      "Epoch 60/200\n",
      "90/90 [==============================] - 0s 266us/step - loss: 0.4290 - acc: 0.9556 - val_loss: 0.5091 - val_acc: 0.9167\n",
      "Epoch 61/200\n",
      "90/90 [==============================] - 0s 233us/step - loss: 0.4251 - acc: 0.9556 - val_loss: 0.5052 - val_acc: 0.8833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/200\n",
      "90/90 [==============================] - 0s 249us/step - loss: 0.4213 - acc: 0.9556 - val_loss: 0.5019 - val_acc: 0.8833\n",
      "Epoch 63/200\n",
      "90/90 [==============================] - 0s 299us/step - loss: 0.4184 - acc: 0.9556 - val_loss: 0.4990 - val_acc: 0.9167\n",
      "Epoch 64/200\n",
      "90/90 [==============================] - 0s 277us/step - loss: 0.4144 - acc: 0.9556 - val_loss: 0.4958 - val_acc: 0.9167\n",
      "Epoch 65/200\n",
      "90/90 [==============================] - 0s 321us/step - loss: 0.4119 - acc: 0.9556 - val_loss: 0.4918 - val_acc: 0.8833\n",
      "Epoch 66/200\n",
      "90/90 [==============================] - 0s 275us/step - loss: 0.4087 - acc: 0.9556 - val_loss: 0.4891 - val_acc: 0.9333\n",
      "Epoch 67/200\n",
      "90/90 [==============================] - 0s 288us/step - loss: 0.4044 - acc: 0.9556 - val_loss: 0.4857 - val_acc: 0.9333\n",
      "Epoch 68/200\n",
      "90/90 [==============================] - 0s 355us/step - loss: 0.4023 - acc: 0.9556 - val_loss: 0.4826 - val_acc: 0.9333\n",
      "Epoch 69/200\n",
      "90/90 [==============================] - 0s 310us/step - loss: 0.3982 - acc: 0.9556 - val_loss: 0.4794 - val_acc: 0.9333\n",
      "Epoch 70/200\n",
      "90/90 [==============================] - 0s 355us/step - loss: 0.3961 - acc: 0.9556 - val_loss: 0.4759 - val_acc: 0.9333\n",
      "Epoch 71/200\n",
      "90/90 [==============================] - 0s 332us/step - loss: 0.3924 - acc: 0.9556 - val_loss: 0.4729 - val_acc: 0.9167\n",
      "Epoch 72/200\n",
      "90/90 [==============================] - 0s 317us/step - loss: 0.3889 - acc: 0.9556 - val_loss: 0.4702 - val_acc: 0.9333\n",
      "Epoch 73/200\n",
      "90/90 [==============================] - 0s 380us/step - loss: 0.3862 - acc: 0.9556 - val_loss: 0.4675 - val_acc: 0.9333\n",
      "Epoch 74/200\n",
      "90/90 [==============================] - 0s 377us/step - loss: 0.3828 - acc: 0.9556 - val_loss: 0.4648 - val_acc: 0.9500\n",
      "Epoch 75/200\n",
      "90/90 [==============================] - 0s 300us/step - loss: 0.3806 - acc: 0.9556 - val_loss: 0.4617 - val_acc: 0.9500\n",
      "Epoch 76/200\n",
      "90/90 [==============================] - 0s 320us/step - loss: 0.3774 - acc: 0.9556 - val_loss: 0.4591 - val_acc: 0.9500\n",
      "Epoch 77/200\n",
      "90/90 [==============================] - 0s 266us/step - loss: 0.3740 - acc: 0.9667 - val_loss: 0.4561 - val_acc: 0.9500\n",
      "Epoch 78/200\n",
      "90/90 [==============================] - 0s 417us/step - loss: 0.3718 - acc: 0.9667 - val_loss: 0.4533 - val_acc: 0.9500\n",
      "Epoch 79/200\n",
      "90/90 [==============================] - 0s 344us/step - loss: 0.3683 - acc: 0.9667 - val_loss: 0.4502 - val_acc: 0.9667\n",
      "Epoch 80/200\n",
      "90/90 [==============================] - 0s 344us/step - loss: 0.3662 - acc: 0.9667 - val_loss: 0.4473 - val_acc: 0.9667\n",
      "Epoch 81/200\n",
      "90/90 [==============================] - 0s 291us/step - loss: 0.3630 - acc: 0.9556 - val_loss: 0.4444 - val_acc: 0.9500\n",
      "Epoch 82/200\n",
      "90/90 [==============================] - 0s 310us/step - loss: 0.3604 - acc: 0.9556 - val_loss: 0.4416 - val_acc: 0.9500\n",
      "Epoch 83/200\n",
      "90/90 [==============================] - 0s 289us/step - loss: 0.3578 - acc: 0.9667 - val_loss: 0.4394 - val_acc: 0.9667\n",
      "Epoch 84/200\n",
      "90/90 [==============================] - 0s 333us/step - loss: 0.3561 - acc: 0.9667 - val_loss: 0.4374 - val_acc: 0.9500\n",
      "Epoch 85/200\n",
      "90/90 [==============================] - 0s 299us/step - loss: 0.3523 - acc: 0.9667 - val_loss: 0.4341 - val_acc: 0.9500\n",
      "Epoch 86/200\n",
      "90/90 [==============================] - 0s 299us/step - loss: 0.3510 - acc: 0.9667 - val_loss: 0.4314 - val_acc: 0.9500\n",
      "Epoch 87/200\n",
      "90/90 [==============================] - 0s 310us/step - loss: 0.3472 - acc: 0.9556 - val_loss: 0.4288 - val_acc: 0.9667\n",
      "Epoch 88/200\n",
      "90/90 [==============================] - 0s 321us/step - loss: 0.3440 - acc: 0.9667 - val_loss: 0.4263 - val_acc: 0.9667\n",
      "Epoch 89/200\n",
      "90/90 [==============================] - 0s 288us/step - loss: 0.3431 - acc: 0.9667 - val_loss: 0.4248 - val_acc: 0.9500\n",
      "Epoch 90/200\n",
      "90/90 [==============================] - 0s 290us/step - loss: 0.3394 - acc: 0.9778 - val_loss: 0.4216 - val_acc: 0.9500\n",
      "Epoch 91/200\n",
      "90/90 [==============================] - 0s 312us/step - loss: 0.3368 - acc: 0.9667 - val_loss: 0.4186 - val_acc: 0.9667\n",
      "Epoch 92/200\n",
      "90/90 [==============================] - 0s 255us/step - loss: 0.3357 - acc: 0.9667 - val_loss: 0.4166 - val_acc: 0.9500\n",
      "Epoch 93/200\n",
      "90/90 [==============================] - 0s 289us/step - loss: 0.3322 - acc: 0.9667 - val_loss: 0.4134 - val_acc: 0.9667\n",
      "Epoch 94/200\n",
      "90/90 [==============================] - 0s 310us/step - loss: 0.3296 - acc: 0.9667 - val_loss: 0.4111 - val_acc: 0.9667\n",
      "Epoch 95/200\n",
      "90/90 [==============================] - 0s 277us/step - loss: 0.3269 - acc: 0.9667 - val_loss: 0.4088 - val_acc: 0.9667\n",
      "Epoch 96/200\n",
      "90/90 [==============================] - 0s 266us/step - loss: 0.3245 - acc: 0.9667 - val_loss: 0.4063 - val_acc: 0.9667\n",
      "Epoch 97/200\n",
      "90/90 [==============================] - 0s 310us/step - loss: 0.3225 - acc: 0.9667 - val_loss: 0.4040 - val_acc: 0.9667\n",
      "Epoch 98/200\n",
      "90/90 [==============================] - 0s 288us/step - loss: 0.3205 - acc: 0.9667 - val_loss: 0.4028 - val_acc: 0.9500\n",
      "Epoch 99/200\n",
      "90/90 [==============================] - 0s 310us/step - loss: 0.3175 - acc: 0.9778 - val_loss: 0.3999 - val_acc: 0.9500\n",
      "Epoch 100/200\n",
      "90/90 [==============================] - 0s 288us/step - loss: 0.3156 - acc: 0.9667 - val_loss: 0.3969 - val_acc: 0.9667\n",
      "Epoch 101/200\n",
      "90/90 [==============================] - 0s 372us/step - loss: 0.3131 - acc: 0.9667 - val_loss: 0.3949 - val_acc: 0.9500\n",
      "Epoch 102/200\n",
      "90/90 [==============================] - 0s 321us/step - loss: 0.3107 - acc: 0.9667 - val_loss: 0.3922 - val_acc: 0.9667\n",
      "Epoch 103/200\n",
      "90/90 [==============================] - 0s 266us/step - loss: 0.3086 - acc: 0.9667 - val_loss: 0.3904 - val_acc: 0.9500\n",
      "Epoch 104/200\n",
      "90/90 [==============================] - 0s 289us/step - loss: 0.3060 - acc: 0.9667 - val_loss: 0.3877 - val_acc: 0.9667\n",
      "Epoch 105/200\n",
      "90/90 [==============================] - 0s 299us/step - loss: 0.3039 - acc: 0.9667 - val_loss: 0.3853 - val_acc: 0.9667\n",
      "Epoch 106/200\n",
      "90/90 [==============================] - 0s 299us/step - loss: 0.3019 - acc: 0.9778 - val_loss: 0.3835 - val_acc: 0.9500\n",
      "Epoch 107/200\n",
      "90/90 [==============================] - 0s 288us/step - loss: 0.2996 - acc: 0.9778 - val_loss: 0.3817 - val_acc: 0.9500\n",
      "Epoch 108/200\n",
      "90/90 [==============================] - 0s 288us/step - loss: 0.2969 - acc: 0.9778 - val_loss: 0.3789 - val_acc: 0.9500\n",
      "Epoch 109/200\n",
      "90/90 [==============================] - 0s 321us/step - loss: 0.2952 - acc: 0.9667 - val_loss: 0.3764 - val_acc: 0.9667\n",
      "Epoch 110/200\n",
      "90/90 [==============================] - 0s 277us/step - loss: 0.2933 - acc: 0.9667 - val_loss: 0.3742 - val_acc: 0.9667\n",
      "Epoch 111/200\n",
      "90/90 [==============================] - 0s 267us/step - loss: 0.2907 - acc: 0.9778 - val_loss: 0.3722 - val_acc: 0.9500\n",
      "Epoch 112/200\n",
      "90/90 [==============================] - 0s 244us/step - loss: 0.2884 - acc: 0.9778 - val_loss: 0.3703 - val_acc: 0.9500\n",
      "Epoch 113/200\n",
      "90/90 [==============================] - 0s 288us/step - loss: 0.2864 - acc: 0.9778 - val_loss: 0.3685 - val_acc: 0.9500\n",
      "Epoch 114/200\n",
      "90/90 [==============================] - 0s 288us/step - loss: 0.2854 - acc: 0.9778 - val_loss: 0.3664 - val_acc: 0.9500\n",
      "Epoch 115/200\n",
      "90/90 [==============================] - 0s 283us/step - loss: 0.2833 - acc: 0.9778 - val_loss: 0.3646 - val_acc: 0.9500\n",
      "Epoch 116/200\n",
      "90/90 [==============================] - 0s 244us/step - loss: 0.2809 - acc: 0.9778 - val_loss: 0.3617 - val_acc: 0.9667\n",
      "Epoch 117/200\n",
      "90/90 [==============================] - 0s 288us/step - loss: 0.2794 - acc: 0.9778 - val_loss: 0.3600 - val_acc: 0.9500\n",
      "Epoch 118/200\n",
      "90/90 [==============================] - 0s 244us/step - loss: 0.2762 - acc: 0.9778 - val_loss: 0.3580 - val_acc: 0.9500\n",
      "Epoch 119/200\n",
      "90/90 [==============================] - 0s 277us/step - loss: 0.2743 - acc: 0.9778 - val_loss: 0.3559 - val_acc: 0.9500\n",
      "Epoch 120/200\n",
      "90/90 [==============================] - 0s 277us/step - loss: 0.2732 - acc: 0.9778 - val_loss: 0.3541 - val_acc: 0.9500\n",
      "Epoch 121/200\n",
      "90/90 [==============================] - 0s 233us/step - loss: 0.2705 - acc: 0.9778 - val_loss: 0.3519 - val_acc: 0.9500\n",
      "Epoch 122/200\n",
      "90/90 [==============================] - 0s 343us/step - loss: 0.2687 - acc: 0.9778 - val_loss: 0.3499 - val_acc: 0.9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/200\n",
      "90/90 [==============================] - 0s 273us/step - loss: 0.2673 - acc: 0.9778 - val_loss: 0.3485 - val_acc: 0.9500\n",
      "Epoch 124/200\n",
      "90/90 [==============================] - 0s 299us/step - loss: 0.2649 - acc: 0.9778 - val_loss: 0.3462 - val_acc: 0.9500\n",
      "Epoch 125/200\n",
      "90/90 [==============================] - 0s 322us/step - loss: 0.2632 - acc: 0.9778 - val_loss: 0.3444 - val_acc: 0.9500\n",
      "Epoch 126/200\n",
      "90/90 [==============================] - 0s 254us/step - loss: 0.2617 - acc: 0.9778 - val_loss: 0.3422 - val_acc: 0.9500\n",
      "Epoch 127/200\n",
      "90/90 [==============================] - 0s 324us/step - loss: 0.2600 - acc: 0.9778 - val_loss: 0.3401 - val_acc: 0.9667\n",
      "Epoch 128/200\n",
      "90/90 [==============================] - 0s 266us/step - loss: 0.2586 - acc: 0.9778 - val_loss: 0.3388 - val_acc: 0.9500\n",
      "Epoch 129/200\n",
      "90/90 [==============================] - 0s 320us/step - loss: 0.2558 - acc: 0.9778 - val_loss: 0.3366 - val_acc: 0.9500\n",
      "Epoch 130/200\n",
      "90/90 [==============================] - 0s 277us/step - loss: 0.2541 - acc: 0.9778 - val_loss: 0.3348 - val_acc: 0.9500\n",
      "Epoch 131/200\n",
      "90/90 [==============================] - 0s 278us/step - loss: 0.2528 - acc: 0.9778 - val_loss: 0.3334 - val_acc: 0.9500\n",
      "Epoch 132/200\n",
      "90/90 [==============================] - 0s 314us/step - loss: 0.2503 - acc: 0.9778 - val_loss: 0.3314 - val_acc: 0.9500\n",
      "Epoch 133/200\n",
      "90/90 [==============================] - 0s 343us/step - loss: 0.2490 - acc: 0.9778 - val_loss: 0.3293 - val_acc: 0.9500\n",
      "Epoch 134/200\n",
      "90/90 [==============================] - 0s 295us/step - loss: 0.2479 - acc: 0.9778 - val_loss: 0.3284 - val_acc: 0.9500\n",
      "Epoch 135/200\n",
      "90/90 [==============================] - 0s 301us/step - loss: 0.2466 - acc: 0.9778 - val_loss: 0.3259 - val_acc: 0.9500\n",
      "Epoch 136/200\n",
      "90/90 [==============================] - 0s 287us/step - loss: 0.2439 - acc: 0.9778 - val_loss: 0.3245 - val_acc: 0.9500\n",
      "Epoch 137/200\n",
      "90/90 [==============================] - 0s 277us/step - loss: 0.2419 - acc: 0.9778 - val_loss: 0.3228 - val_acc: 0.9500\n",
      "Epoch 138/200\n",
      "90/90 [==============================] - 0s 310us/step - loss: 0.2404 - acc: 0.9778 - val_loss: 0.3208 - val_acc: 0.9500\n",
      "Epoch 139/200\n",
      "90/90 [==============================] - 0s 327us/step - loss: 0.2389 - acc: 0.9778 - val_loss: 0.3189 - val_acc: 0.9500\n",
      "Epoch 140/200\n",
      "90/90 [==============================] - 0s 355us/step - loss: 0.2378 - acc: 0.9778 - val_loss: 0.3181 - val_acc: 0.9667\n",
      "Epoch 141/200\n",
      "90/90 [==============================] - 0s 321us/step - loss: 0.2357 - acc: 0.9778 - val_loss: 0.3157 - val_acc: 0.9500\n",
      "Epoch 142/200\n",
      "90/90 [==============================] - 0s 301us/step - loss: 0.2339 - acc: 0.9778 - val_loss: 0.3139 - val_acc: 0.9500\n",
      "Epoch 143/200\n",
      "90/90 [==============================] - 0s 355us/step - loss: 0.2331 - acc: 0.9778 - val_loss: 0.3129 - val_acc: 0.9667\n",
      "Epoch 144/200\n",
      "90/90 [==============================] - 0s 299us/step - loss: 0.2308 - acc: 0.9778 - val_loss: 0.3113 - val_acc: 0.9667\n",
      "Epoch 145/200\n",
      "90/90 [==============================] - 0s 292us/step - loss: 0.2293 - acc: 0.9778 - val_loss: 0.3088 - val_acc: 0.9667\n",
      "Epoch 146/200\n",
      "90/90 [==============================] - 0s 333us/step - loss: 0.2275 - acc: 0.9778 - val_loss: 0.3074 - val_acc: 0.9500\n",
      "Epoch 147/200\n",
      "90/90 [==============================] - 0s 436us/step - loss: 0.2262 - acc: 0.9778 - val_loss: 0.3060 - val_acc: 0.9667\n",
      "Epoch 148/200\n",
      "90/90 [==============================] - 0s 314us/step - loss: 0.2253 - acc: 0.9778 - val_loss: 0.3046 - val_acc: 0.9667\n",
      "Epoch 149/200\n",
      "90/90 [==============================] - 0s 316us/step - loss: 0.2232 - acc: 0.9778 - val_loss: 0.3025 - val_acc: 0.9500\n",
      "Epoch 150/200\n",
      "90/90 [==============================] - 0s 332us/step - loss: 0.2216 - acc: 0.9778 - val_loss: 0.3009 - val_acc: 0.9500\n",
      "Epoch 151/200\n",
      "90/90 [==============================] - 0s 294us/step - loss: 0.2203 - acc: 0.9778 - val_loss: 0.2994 - val_acc: 0.9500\n",
      "Epoch 152/200\n",
      "90/90 [==============================] - 0s 300us/step - loss: 0.2184 - acc: 0.9778 - val_loss: 0.2981 - val_acc: 0.9667\n",
      "Epoch 153/200\n",
      "90/90 [==============================] - 0s 277us/step - loss: 0.2169 - acc: 0.9778 - val_loss: 0.2968 - val_acc: 0.9667\n",
      "Epoch 154/200\n",
      "90/90 [==============================] - 0s 265us/step - loss: 0.2161 - acc: 0.9778 - val_loss: 0.2953 - val_acc: 0.9667\n",
      "Epoch 155/200\n",
      "90/90 [==============================] - 0s 277us/step - loss: 0.2145 - acc: 0.9778 - val_loss: 0.2937 - val_acc: 0.9667\n",
      "Epoch 156/200\n",
      "90/90 [==============================] - 0s 310us/step - loss: 0.2129 - acc: 0.9778 - val_loss: 0.2922 - val_acc: 0.9667\n",
      "Epoch 157/200\n",
      "90/90 [==============================] - 0s 299us/step - loss: 0.2116 - acc: 0.9778 - val_loss: 0.2904 - val_acc: 0.9667\n",
      "Epoch 158/200\n",
      "90/90 [==============================] - 0s 266us/step - loss: 0.2105 - acc: 0.9778 - val_loss: 0.2892 - val_acc: 0.9667\n",
      "Epoch 159/200\n",
      "90/90 [==============================] - 0s 300us/step - loss: 0.2090 - acc: 0.9778 - val_loss: 0.2877 - val_acc: 0.9667\n",
      "Epoch 160/200\n",
      "90/90 [==============================] - 0s 277us/step - loss: 0.2085 - acc: 0.9778 - val_loss: 0.2858 - val_acc: 0.9667\n",
      "Epoch 161/200\n",
      "90/90 [==============================] - 0s 416us/step - loss: 0.2070 - acc: 0.9778 - val_loss: 0.2849 - val_acc: 0.9667\n",
      "Epoch 162/200\n",
      "90/90 [==============================] - 0s 332us/step - loss: 0.2046 - acc: 0.9778 - val_loss: 0.2834 - val_acc: 0.9667\n",
      "Epoch 163/200\n",
      "90/90 [==============================] - 0s 332us/step - loss: 0.2034 - acc: 0.9778 - val_loss: 0.2823 - val_acc: 0.9667\n",
      "Epoch 164/200\n",
      "90/90 [==============================] - 0s 361us/step - loss: 0.2022 - acc: 0.9778 - val_loss: 0.2806 - val_acc: 0.9667\n",
      "Epoch 165/200\n",
      "90/90 [==============================] - 0s 332us/step - loss: 0.2006 - acc: 0.9778 - val_loss: 0.2791 - val_acc: 0.9667\n",
      "Epoch 166/200\n",
      "90/90 [==============================] - 0s 377us/step - loss: 0.1998 - acc: 0.9778 - val_loss: 0.2776 - val_acc: 0.9667\n",
      "Epoch 167/200\n",
      "90/90 [==============================] - 0s 257us/step - loss: 0.1987 - acc: 0.9778 - val_loss: 0.2760 - val_acc: 0.9833\n",
      "Epoch 168/200\n",
      "90/90 [==============================] - 0s 265us/step - loss: 0.1968 - acc: 0.9778 - val_loss: 0.2750 - val_acc: 0.9667\n",
      "Epoch 169/200\n",
      "90/90 [==============================] - 0s 255us/step - loss: 0.1974 - acc: 0.9778 - val_loss: 0.2752 - val_acc: 0.9500\n",
      "Epoch 170/200\n",
      "90/90 [==============================] - 0s 278us/step - loss: 0.1947 - acc: 0.9778 - val_loss: 0.2735 - val_acc: 0.9667\n",
      "Epoch 171/200\n",
      "90/90 [==============================] - 0s 266us/step - loss: 0.1936 - acc: 0.9778 - val_loss: 0.2709 - val_acc: 0.9667\n",
      "Epoch 172/200\n",
      "90/90 [==============================] - 0s 310us/step - loss: 0.1928 - acc: 0.9778 - val_loss: 0.2694 - val_acc: 0.9833\n",
      "Epoch 173/200\n",
      "90/90 [==============================] - 0s 399us/step - loss: 0.1908 - acc: 0.9778 - val_loss: 0.2686 - val_acc: 0.9667\n",
      "Epoch 174/200\n",
      "90/90 [==============================] - 0s 267us/step - loss: 0.1898 - acc: 0.9778 - val_loss: 0.2680 - val_acc: 0.9667\n",
      "Epoch 175/200\n",
      "90/90 [==============================] - 0s 355us/step - loss: 0.1889 - acc: 0.9778 - val_loss: 0.2661 - val_acc: 0.9667\n",
      "Epoch 176/200\n",
      "90/90 [==============================] - 0s 277us/step - loss: 0.1882 - acc: 0.9778 - val_loss: 0.2644 - val_acc: 0.9833\n",
      "Epoch 177/200\n",
      "90/90 [==============================] - 0s 288us/step - loss: 0.1860 - acc: 0.9778 - val_loss: 0.2634 - val_acc: 0.9667\n",
      "Epoch 178/200\n",
      "90/90 [==============================] - 0s 277us/step - loss: 0.1849 - acc: 0.9778 - val_loss: 0.2630 - val_acc: 0.9667\n",
      "Epoch 179/200\n",
      "90/90 [==============================] - 0s 298us/step - loss: 0.1850 - acc: 0.9778 - val_loss: 0.2626 - val_acc: 0.9500\n",
      "Epoch 180/200\n",
      "90/90 [==============================] - 0s 281us/step - loss: 0.1837 - acc: 0.9778 - val_loss: 0.2598 - val_acc: 0.9667\n",
      "Epoch 181/200\n",
      "90/90 [==============================] - 0s 277us/step - loss: 0.1827 - acc: 0.9778 - val_loss: 0.2583 - val_acc: 0.9833\n",
      "Epoch 182/200\n",
      "90/90 [==============================] - 0s 277us/step - loss: 0.1806 - acc: 0.9778 - val_loss: 0.2574 - val_acc: 0.9667\n",
      "Epoch 183/200\n",
      "90/90 [==============================] - 0s 344us/step - loss: 0.1794 - acc: 0.9778 - val_loss: 0.2566 - val_acc: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184/200\n",
      "90/90 [==============================] - 0s 288us/step - loss: 0.1789 - acc: 0.9778 - val_loss: 0.2560 - val_acc: 0.9667\n",
      "Epoch 185/200\n",
      "90/90 [==============================] - 0s 299us/step - loss: 0.1779 - acc: 0.9778 - val_loss: 0.2551 - val_acc: 0.9667\n",
      "Epoch 186/200\n",
      "90/90 [==============================] - 0s 249us/step - loss: 0.1761 - acc: 0.9778 - val_loss: 0.2530 - val_acc: 0.9667\n",
      "Epoch 187/200\n",
      "90/90 [==============================] - 0s 333us/step - loss: 0.1752 - acc: 0.9778 - val_loss: 0.2514 - val_acc: 0.9833\n",
      "Epoch 188/200\n",
      "90/90 [==============================] - 0s 300us/step - loss: 0.1743 - acc: 0.9778 - val_loss: 0.2502 - val_acc: 0.9833\n",
      "Epoch 189/200\n",
      "90/90 [==============================] - 0s 299us/step - loss: 0.1732 - acc: 0.9778 - val_loss: 0.2494 - val_acc: 0.9667\n",
      "Epoch 190/200\n",
      "90/90 [==============================] - 0s 342us/step - loss: 0.1720 - acc: 0.9778 - val_loss: 0.2486 - val_acc: 0.9667\n",
      "Epoch 191/200\n",
      "90/90 [==============================] - 0s 366us/step - loss: 0.1715 - acc: 0.9778 - val_loss: 0.2481 - val_acc: 0.9667\n",
      "Epoch 192/200\n",
      "90/90 [==============================] - 0s 299us/step - loss: 0.1713 - acc: 0.9778 - val_loss: 0.2462 - val_acc: 0.9667\n",
      "Epoch 193/200\n",
      "90/90 [==============================] - 0s 330us/step - loss: 0.1698 - acc: 0.9778 - val_loss: 0.2459 - val_acc: 0.9667\n",
      "Epoch 194/200\n",
      "90/90 [==============================] - 0s 294us/step - loss: 0.1681 - acc: 0.9778 - val_loss: 0.2441 - val_acc: 0.9667\n",
      "Epoch 195/200\n",
      "90/90 [==============================] - 0s 325us/step - loss: 0.1672 - acc: 0.9778 - val_loss: 0.2431 - val_acc: 0.9667\n",
      "Epoch 196/200\n",
      "90/90 [==============================] - 0s 308us/step - loss: 0.1666 - acc: 0.9778 - val_loss: 0.2422 - val_acc: 0.9667\n",
      "Epoch 197/200\n",
      "90/90 [==============================] - 0s 296us/step - loss: 0.1653 - acc: 0.9778 - val_loss: 0.2410 - val_acc: 0.9667\n",
      "Epoch 198/200\n",
      "90/90 [==============================] - 0s 355us/step - loss: 0.1643 - acc: 0.9778 - val_loss: 0.2398 - val_acc: 0.9667\n",
      "Epoch 199/200\n",
      "90/90 [==============================] - 0s 288us/step - loss: 0.1640 - acc: 0.9778 - val_loss: 0.2392 - val_acc: 0.9667\n",
      "Epoch 200/200\n",
      "90/90 [==============================] - 0s 288us/step - loss: 0.1625 - acc: 0.9778 - val_loss: 0.2376 - val_acc: 0.9667\n"
     ]
    }
   ],
   "source": [
    " history = model.fit(x_train , y_train, validation_data = (x_test ,y_test),epochs=200 , batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 0s 214us/step\n",
      "test_acc: 0.9666666547457378\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
