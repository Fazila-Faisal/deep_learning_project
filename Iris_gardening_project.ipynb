{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi_class Classification Iris Flowers Project :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# objective:Botanist wants to determine the species of an iris flower based on characteristics of that flower.For instance attributes including petal,petal width,sepal length,sepal width are \"features\" that determine the classification of a given iris flower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start off by importing all of the classes and functions we will need. This includes both the functionality we require from Keras, but also data loading from pandas as well as data preparation and model evaluation from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the iris dataset  into scikit_learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import load_iris function from dataset module\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save bunch object containing iris_dataset and iris attributes\n",
    "data_frame_iris  = load_iris()\n",
    "type(data_frame_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n"
     ]
    }
   ],
   "source": [
    "# print the iris data\n",
    "print(data_frame_iris.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning Termonolgy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " •\tEach row is an observation is (also known as: samples, instance   ).\n",
    " \n",
    " •\tEach column is a feature (also know as : predicator, attributes, input ,covariate).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "# print the names of features\n",
    "print(data_frame_iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "# print the integers the species of each observation\n",
    "print(data_frame_iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# print encoding scheme for sepecies: 0 = \"setosa\", 1 = \"versicolor\", 2 = \"virginica\"\n",
    "print(data_frame_iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " •\tEach  value we are predicting is the response (also known as: target ,outcomes,label,dependent variable).\n",
    " \n",
    " •\tClassification is supervised learning in which the response is categorical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# check the type of featurs and response\n",
    "print(type(data_frame_iris.data))\n",
    "print(type(data_frame_iris.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "# check the shape of features and response\n",
    "print(data_frame_iris.data.shape)\n",
    "print(data_frame_iris.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n"
     ]
    }
   ],
   "source": [
    "# store feature matrix in x\n",
    "x =data_frame_iris[\"data\"]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# store response matrix in y\n",
    "y =to_categorical(data_frame_iris[\"target\"])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data  and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.7 2.6 3.5 1. ]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [5.  3.  1.6 0.2]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.  3.  4.8 1.8]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [4.6 3.1 1.5 0.2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_train)\n",
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.7 4.4 1.5 0.4]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [6.  2.2 5.  1.5]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [5.  2.  3.5 1. ]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.3 2.3 4.4 1.3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_test)\n",
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train)\n",
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_test)\n",
    "len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(6,input_dim = 4 ,activation = \"relu\"))\n",
    "model.add(layers.Dense(3,activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0713 00:02:01.669674  6380 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0713 00:02:01.810619  6380 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 45 samples\n",
      "Epoch 1/200\n",
      "105/105 [==============================] - 0s 4ms/step - loss: 2.6989 - acc: 0.3619 - val_loss: 2.7058 - val_acc: 0.2667\n",
      "Epoch 2/200\n",
      "105/105 [==============================] - 0s 465us/step - loss: 2.3022 - acc: 0.3619 - val_loss: 2.3727 - val_acc: 0.2667\n",
      "Epoch 3/200\n",
      "105/105 [==============================] - 0s 513us/step - loss: 2.0351 - acc: 0.3619 - val_loss: 2.1137 - val_acc: 0.2667\n",
      "Epoch 4/200\n",
      "105/105 [==============================] - 0s 579us/step - loss: 1.8216 - acc: 0.3619 - val_loss: 1.9034 - val_acc: 0.2667\n",
      "Epoch 5/200\n",
      "105/105 [==============================] - 0s 531us/step - loss: 1.6470 - acc: 0.3619 - val_loss: 1.7226 - val_acc: 0.2667\n",
      "Epoch 6/200\n",
      "105/105 [==============================] - 0s 466us/step - loss: 1.4976 - acc: 0.3619 - val_loss: 1.5751 - val_acc: 0.2667\n",
      "Epoch 7/200\n",
      "105/105 [==============================] - 0s 690us/step - loss: 1.3745 - acc: 0.3619 - val_loss: 1.4503 - val_acc: 0.2667\n",
      "Epoch 8/200\n",
      "105/105 [==============================] - 0s 628us/step - loss: 1.2724 - acc: 0.3619 - val_loss: 1.3448 - val_acc: 0.2667\n",
      "Epoch 9/200\n",
      "105/105 [==============================] - 0s 551us/step - loss: 1.1897 - acc: 0.3619 - val_loss: 1.2600 - val_acc: 0.2667\n",
      "Epoch 10/200\n",
      "105/105 [==============================] - 0s 494us/step - loss: 1.1226 - acc: 0.3619 - val_loss: 1.1874 - val_acc: 0.2667\n",
      "Epoch 11/200\n",
      "105/105 [==============================] - 0s 893us/step - loss: 1.0641 - acc: 0.3714 - val_loss: 1.1273 - val_acc: 0.2667\n",
      "Epoch 12/200\n",
      "105/105 [==============================] - 0s 494us/step - loss: 1.0136 - acc: 0.4190 - val_loss: 1.0755 - val_acc: 0.2889\n",
      "Epoch 13/200\n",
      "105/105 [==============================] - 0s 408us/step - loss: 0.9706 - acc: 0.4667 - val_loss: 1.0303 - val_acc: 0.4222\n",
      "Epoch 14/200\n",
      "105/105 [==============================] - 0s 522us/step - loss: 0.9313 - acc: 0.4952 - val_loss: 0.9917 - val_acc: 0.5333\n",
      "Epoch 15/200\n",
      "105/105 [==============================] - 0s 503us/step - loss: 0.8983 - acc: 0.6095 - val_loss: 0.9573 - val_acc: 0.6000\n",
      "Epoch 16/200\n",
      "105/105 [==============================] - 0s 285us/step - loss: 0.8674 - acc: 0.6190 - val_loss: 0.9242 - val_acc: 0.6444\n",
      "Epoch 17/200\n",
      "105/105 [==============================] - 0s 342us/step - loss: 0.8377 - acc: 0.6667 - val_loss: 0.8946 - val_acc: 0.6444\n",
      "Epoch 18/200\n",
      "105/105 [==============================] - 0s 266us/step - loss: 0.8114 - acc: 0.6667 - val_loss: 0.8694 - val_acc: 0.6444\n",
      "Epoch 19/200\n",
      "105/105 [==============================] - 0s 254us/step - loss: 0.7903 - acc: 0.6762 - val_loss: 0.8461 - val_acc: 0.6444\n",
      "Epoch 20/200\n",
      "105/105 [==============================] - 0s 391us/step - loss: 0.7689 - acc: 0.6762 - val_loss: 0.8243 - val_acc: 0.6444\n",
      "Epoch 21/200\n",
      "105/105 [==============================] - 0s 491us/step - loss: 0.7492 - acc: 0.6762 - val_loss: 0.8053 - val_acc: 0.6444\n",
      "Epoch 22/200\n",
      "105/105 [==============================] - 0s 304us/step - loss: 0.7324 - acc: 0.6762 - val_loss: 0.7879 - val_acc: 0.6444\n",
      "Epoch 23/200\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.7161 - acc: 0.6762 - val_loss: 0.7717 - val_acc: 0.6444\n",
      "Epoch 24/200\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.7022 - acc: 0.6762 - val_loss: 0.7570 - val_acc: 0.6667\n",
      "Epoch 25/200\n",
      "105/105 [==============================] - 0s 192us/step - loss: 0.6881 - acc: 0.6762 - val_loss: 0.7427 - val_acc: 0.6667\n",
      "Epoch 26/200\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.6749 - acc: 0.6762 - val_loss: 0.7290 - val_acc: 0.7333\n",
      "Epoch 27/200\n",
      "105/105 [==============================] - 0s 237us/step - loss: 0.6627 - acc: 0.6762 - val_loss: 0.7193 - val_acc: 0.7556\n",
      "Epoch 28/200\n",
      "105/105 [==============================] - 0s 235us/step - loss: 0.6523 - acc: 0.7048 - val_loss: 0.7084 - val_acc: 0.7556\n",
      "Epoch 29/200\n",
      "105/105 [==============================] - 0s 362us/step - loss: 0.6413 - acc: 0.7429 - val_loss: 0.6934 - val_acc: 0.7778\n",
      "Epoch 30/200\n",
      "105/105 [==============================] - 0s 294us/step - loss: 0.6312 - acc: 0.7429 - val_loss: 0.6850 - val_acc: 0.8444\n",
      "Epoch 31/200\n",
      "105/105 [==============================] - 0s 294us/step - loss: 0.6200 - acc: 0.7619 - val_loss: 0.6740 - val_acc: 0.9111\n",
      "Epoch 32/200\n",
      "105/105 [==============================] - 0s 266us/step - loss: 0.6105 - acc: 0.8476 - val_loss: 0.6579 - val_acc: 0.8889\n",
      "Epoch 33/200\n",
      "105/105 [==============================] - 0s 285us/step - loss: 0.6012 - acc: 0.8476 - val_loss: 0.6509 - val_acc: 0.8889\n",
      "Epoch 34/200\n",
      "105/105 [==============================] - 0s 275us/step - loss: 0.5925 - acc: 0.8190 - val_loss: 0.6419 - val_acc: 0.9111\n",
      "Epoch 35/200\n",
      "105/105 [==============================] - 0s 238us/step - loss: 0.5844 - acc: 0.8476 - val_loss: 0.6314 - val_acc: 0.9111\n",
      "Epoch 36/200\n",
      "105/105 [==============================] - 0s 266us/step - loss: 0.5746 - acc: 0.8667 - val_loss: 0.6223 - val_acc: 0.9333\n",
      "Epoch 37/200\n",
      "105/105 [==============================] - 0s 361us/step - loss: 0.5651 - acc: 0.9048 - val_loss: 0.6090 - val_acc: 0.9333\n",
      "Epoch 38/200\n",
      "105/105 [==============================] - 0s 275us/step - loss: 0.5564 - acc: 0.8952 - val_loss: 0.5994 - val_acc: 0.9333\n",
      "Epoch 39/200\n",
      "105/105 [==============================] - 0s 278us/step - loss: 0.5470 - acc: 0.9238 - val_loss: 0.5878 - val_acc: 0.9556\n",
      "Epoch 40/200\n",
      "105/105 [==============================] - 0s 275us/step - loss: 0.5383 - acc: 0.9238 - val_loss: 0.5780 - val_acc: 0.9556\n",
      "Epoch 41/200\n",
      "105/105 [==============================] - 0s 256us/step - loss: 0.5308 - acc: 0.9333 - val_loss: 0.5685 - val_acc: 0.9556\n",
      "Epoch 42/200\n",
      "105/105 [==============================] - 0s 199us/step - loss: 0.5218 - acc: 0.9333 - val_loss: 0.5631 - val_acc: 0.9556\n",
      "Epoch 43/200\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.5143 - acc: 0.9619 - val_loss: 0.5536 - val_acc: 0.9556\n",
      "Epoch 44/200\n",
      "105/105 [==============================] - 0s 218us/step - loss: 0.5058 - acc: 0.9429 - val_loss: 0.5447 - val_acc: 0.9556\n",
      "Epoch 45/200\n",
      "105/105 [==============================] - 0s 408us/step - loss: 0.4980 - acc: 0.9333 - val_loss: 0.5363 - val_acc: 0.9556\n",
      "Epoch 46/200\n",
      "105/105 [==============================] - 0s 351us/step - loss: 0.4909 - acc: 0.9429 - val_loss: 0.5262 - val_acc: 0.9556\n",
      "Epoch 47/200\n",
      "105/105 [==============================] - 0s 237us/step - loss: 0.4818 - acc: 0.9429 - val_loss: 0.5190 - val_acc: 0.9556\n",
      "Epoch 48/200\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.4766 - acc: 0.9619 - val_loss: 0.5108 - val_acc: 0.9556\n",
      "Epoch 49/200\n",
      "105/105 [==============================] - 0s 239us/step - loss: 0.4694 - acc: 0.9429 - val_loss: 0.5022 - val_acc: 0.9556\n",
      "Epoch 50/200\n",
      "105/105 [==============================] - 0s 256us/step - loss: 0.4627 - acc: 0.9524 - val_loss: 0.4956 - val_acc: 0.9556\n",
      "Epoch 51/200\n",
      "105/105 [==============================] - 0s 218us/step - loss: 0.4558 - acc: 0.9429 - val_loss: 0.4884 - val_acc: 0.9556\n",
      "Epoch 52/200\n",
      "105/105 [==============================] - 0s 380us/step - loss: 0.4495 - acc: 0.9524 - val_loss: 0.4804 - val_acc: 0.9556\n",
      "Epoch 53/200\n",
      "105/105 [==============================] - 0s 266us/step - loss: 0.4428 - acc: 0.9524 - val_loss: 0.4736 - val_acc: 0.9556\n",
      "Epoch 54/200\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.4369 - acc: 0.9524 - val_loss: 0.4667 - val_acc: 0.9556\n",
      "Epoch 55/200\n",
      "105/105 [==============================] - 0s 266us/step - loss: 0.4306 - acc: 0.9714 - val_loss: 0.4605 - val_acc: 0.9556\n",
      "Epoch 56/200\n",
      "105/105 [==============================] - 0s 233us/step - loss: 0.4251 - acc: 0.9619 - val_loss: 0.4558 - val_acc: 0.9556\n",
      "Epoch 57/200\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.4198 - acc: 0.9619 - val_loss: 0.4487 - val_acc: 0.9556\n",
      "Epoch 58/200\n",
      "105/105 [==============================] - 0s 218us/step - loss: 0.4139 - acc: 0.9619 - val_loss: 0.4418 - val_acc: 0.9556\n",
      "Epoch 59/200\n",
      "105/105 [==============================] - 0s 313us/step - loss: 0.4086 - acc: 0.9619 - val_loss: 0.4353 - val_acc: 0.9556\n",
      "Epoch 60/200\n",
      "105/105 [==============================] - 0s 275us/step - loss: 0.4027 - acc: 0.9714 - val_loss: 0.4311 - val_acc: 0.9556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.3979 - acc: 0.9619 - val_loss: 0.4253 - val_acc: 0.9556\n",
      "Epoch 62/200\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.3947 - acc: 0.9619 - val_loss: 0.4205 - val_acc: 0.9556\n",
      "Epoch 63/200\n",
      "105/105 [==============================] - 0s 275us/step - loss: 0.3888 - acc: 0.9619 - val_loss: 0.4139 - val_acc: 0.9556\n",
      "Epoch 64/200\n",
      "105/105 [==============================] - 0s 263us/step - loss: 0.3846 - acc: 0.9619 - val_loss: 0.4096 - val_acc: 0.9556\n",
      "Epoch 65/200\n",
      "105/105 [==============================] - 0s 235us/step - loss: 0.3800 - acc: 0.9619 - val_loss: 0.4060 - val_acc: 0.9556\n",
      "Epoch 66/200\n",
      "105/105 [==============================] - 0s 266us/step - loss: 0.3767 - acc: 0.9429 - val_loss: 0.3978 - val_acc: 0.9556\n",
      "Epoch 67/200\n",
      "105/105 [==============================] - 0s 408us/step - loss: 0.3712 - acc: 0.9619 - val_loss: 0.3942 - val_acc: 0.9556\n",
      "Epoch 68/200\n",
      "105/105 [==============================] - 0s 266us/step - loss: 0.3684 - acc: 0.9619 - val_loss: 0.3897 - val_acc: 0.9556\n",
      "Epoch 69/200\n",
      "105/105 [==============================] - 0s 237us/step - loss: 0.3655 - acc: 0.9619 - val_loss: 0.3874 - val_acc: 0.9556\n",
      "Epoch 70/200\n",
      "105/105 [==============================] - 0s 218us/step - loss: 0.3597 - acc: 0.9619 - val_loss: 0.3831 - val_acc: 0.9556\n",
      "Epoch 71/200\n",
      "105/105 [==============================] - 0s 199us/step - loss: 0.3566 - acc: 0.9619 - val_loss: 0.3794 - val_acc: 0.9556\n",
      "Epoch 72/200\n",
      "105/105 [==============================] - 0s 199us/step - loss: 0.3526 - acc: 0.9619 - val_loss: 0.3767 - val_acc: 0.9778\n",
      "Epoch 73/200\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.3494 - acc: 0.9714 - val_loss: 0.3719 - val_acc: 0.9778\n",
      "Epoch 74/200\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.3461 - acc: 0.9619 - val_loss: 0.3661 - val_acc: 0.9556\n",
      "Epoch 75/200\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.3424 - acc: 0.9619 - val_loss: 0.3620 - val_acc: 0.9556\n",
      "Epoch 76/200\n",
      "105/105 [==============================] - 0s 294us/step - loss: 0.3394 - acc: 0.9619 - val_loss: 0.3601 - val_acc: 0.9778\n",
      "Epoch 77/200\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.3358 - acc: 0.9619 - val_loss: 0.3549 - val_acc: 0.9556\n",
      "Epoch 78/200\n",
      "105/105 [==============================] - 0s 261us/step - loss: 0.3322 - acc: 0.9619 - val_loss: 0.3504 - val_acc: 0.9556\n",
      "Epoch 79/200\n",
      "105/105 [==============================] - 0s 266us/step - loss: 0.3305 - acc: 0.9333 - val_loss: 0.3476 - val_acc: 0.9778\n",
      "Epoch 80/200\n",
      "105/105 [==============================] - 0s 245us/step - loss: 0.3260 - acc: 0.9619 - val_loss: 0.3438 - val_acc: 0.9556\n",
      "Epoch 81/200\n",
      "105/105 [==============================] - 0s 237us/step - loss: 0.3241 - acc: 0.9714 - val_loss: 0.3427 - val_acc: 0.9778\n",
      "Epoch 82/200\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.3211 - acc: 0.9619 - val_loss: 0.3382 - val_acc: 0.9556\n",
      "Epoch 83/200\n",
      "105/105 [==============================] - 0s 237us/step - loss: 0.3180 - acc: 0.9619 - val_loss: 0.3352 - val_acc: 0.9556\n",
      "Epoch 84/200\n",
      "105/105 [==============================] - 0s 238us/step - loss: 0.3159 - acc: 0.9619 - val_loss: 0.3317 - val_acc: 0.9556\n",
      "Epoch 85/200\n",
      "105/105 [==============================] - 0s 304us/step - loss: 0.3124 - acc: 0.9619 - val_loss: 0.3296 - val_acc: 0.9556\n",
      "Epoch 86/200\n",
      "105/105 [==============================] - 0s 266us/step - loss: 0.3103 - acc: 0.9619 - val_loss: 0.3261 - val_acc: 0.9556\n",
      "Epoch 87/200\n",
      "105/105 [==============================] - 0s 237us/step - loss: 0.3077 - acc: 0.9619 - val_loss: 0.3241 - val_acc: 0.9778\n",
      "Epoch 88/200\n",
      "105/105 [==============================] - 0s 218us/step - loss: 0.3066 - acc: 0.9619 - val_loss: 0.3225 - val_acc: 0.9778\n",
      "Epoch 89/200\n",
      "105/105 [==============================] - 0s 199us/step - loss: 0.3034 - acc: 0.9524 - val_loss: 0.3188 - val_acc: 0.9778\n",
      "Epoch 90/200\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.3030 - acc: 0.9524 - val_loss: 0.3166 - val_acc: 0.9778\n",
      "Epoch 91/200\n",
      "105/105 [==============================] - 0s 243us/step - loss: 0.2992 - acc: 0.9619 - val_loss: 0.3142 - val_acc: 0.9778\n",
      "Epoch 92/200\n",
      "105/105 [==============================] - 0s 189us/step - loss: 0.2965 - acc: 0.9619 - val_loss: 0.3117 - val_acc: 0.9778\n",
      "Epoch 93/200\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.2951 - acc: 0.9619 - val_loss: 0.3080 - val_acc: 0.9778\n",
      "Epoch 94/200\n",
      "105/105 [==============================] - 0s 351us/step - loss: 0.2928 - acc: 0.9524 - val_loss: 0.3047 - val_acc: 0.9556\n",
      "Epoch 95/200\n",
      "105/105 [==============================] - 0s 294us/step - loss: 0.2901 - acc: 0.9619 - val_loss: 0.3017 - val_acc: 0.9556\n",
      "Epoch 96/200\n",
      "105/105 [==============================] - 0s 232us/step - loss: 0.2895 - acc: 0.9619 - val_loss: 0.3002 - val_acc: 0.9778\n",
      "Epoch 97/200\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.2857 - acc: 0.9619 - val_loss: 0.2981 - val_acc: 0.9778\n",
      "Epoch 98/200\n",
      "105/105 [==============================] - 0s 237us/step - loss: 0.2847 - acc: 0.9524 - val_loss: 0.2949 - val_acc: 0.9778\n",
      "Epoch 99/200\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.2829 - acc: 0.9619 - val_loss: 0.2928 - val_acc: 0.9778\n",
      "Epoch 100/200\n",
      "105/105 [==============================] - 0s 248us/step - loss: 0.2809 - acc: 0.9619 - val_loss: 0.2897 - val_acc: 0.9556\n",
      "Epoch 101/200\n",
      "105/105 [==============================] - 0s 169us/step - loss: 0.2781 - acc: 0.9619 - val_loss: 0.2874 - val_acc: 0.9556\n",
      "Epoch 102/200\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.2780 - acc: 0.9619 - val_loss: 0.2858 - val_acc: 0.9778\n",
      "Epoch 103/200\n",
      "105/105 [==============================] - 0s 251us/step - loss: 0.2741 - acc: 0.9619 - val_loss: 0.2845 - val_acc: 0.9778\n",
      "Epoch 104/200\n",
      "105/105 [==============================] - 0s 503us/step - loss: 0.2738 - acc: 0.9619 - val_loss: 0.2812 - val_acc: 0.9778\n",
      "Epoch 105/200\n",
      "105/105 [==============================] - 0s 636us/step - loss: 0.2710 - acc: 0.9619 - val_loss: 0.2786 - val_acc: 0.9556\n",
      "Epoch 106/200\n",
      "105/105 [==============================] - 0s 285us/step - loss: 0.2701 - acc: 0.9619 - val_loss: 0.2768 - val_acc: 0.9556\n",
      "Epoch 107/200\n",
      "105/105 [==============================] - 0s 304us/step - loss: 0.2673 - acc: 0.9619 - val_loss: 0.2761 - val_acc: 0.9778\n",
      "Epoch 108/200\n",
      "105/105 [==============================] - 0s 418us/step - loss: 0.2670 - acc: 0.9619 - val_loss: 0.2734 - val_acc: 0.9778\n",
      "Epoch 109/200\n",
      "105/105 [==============================] - 0s 484us/step - loss: 0.2639 - acc: 0.9619 - val_loss: 0.2710 - val_acc: 0.9556\n",
      "Epoch 110/200\n",
      "105/105 [==============================] - 0s 308us/step - loss: 0.2630 - acc: 0.9619 - val_loss: 0.2700 - val_acc: 0.9778\n",
      "Epoch 111/200\n",
      "105/105 [==============================] - 0s 295us/step - loss: 0.2619 - acc: 0.9524 - val_loss: 0.2684 - val_acc: 0.9778\n",
      "Epoch 112/200\n",
      "105/105 [==============================] - 0s 303us/step - loss: 0.2599 - acc: 0.9619 - val_loss: 0.2655 - val_acc: 0.9778\n",
      "Epoch 113/200\n",
      "105/105 [==============================] - 0s 286us/step - loss: 0.2586 - acc: 0.9619 - val_loss: 0.2640 - val_acc: 0.9778\n",
      "Epoch 114/200\n",
      "105/105 [==============================] - 0s 313us/step - loss: 0.2559 - acc: 0.9619 - val_loss: 0.2622 - val_acc: 0.9778\n",
      "Epoch 115/200\n",
      "105/105 [==============================] - 0s 266us/step - loss: 0.2545 - acc: 0.9619 - val_loss: 0.2593 - val_acc: 0.9778\n",
      "Epoch 116/200\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.1864 - acc: 1.000 - 0s 361us/step - loss: 0.2523 - acc: 0.9524 - val_loss: 0.2567 - val_acc: 0.9556\n",
      "Epoch 117/200\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.2514 - acc: 0.9619 - val_loss: 0.2547 - val_acc: 0.9778\n",
      "Epoch 118/200\n",
      "105/105 [==============================] - 0s 199us/step - loss: 0.2495 - acc: 0.9619 - val_loss: 0.2527 - val_acc: 0.9778\n",
      "Epoch 119/200\n",
      "105/105 [==============================] - 0s 199us/step - loss: 0.2474 - acc: 0.9619 - val_loss: 0.2508 - val_acc: 0.9778\n",
      "Epoch 120/200\n",
      "105/105 [==============================] - 0s 189us/step - loss: 0.2466 - acc: 0.9619 - val_loss: 0.2493 - val_acc: 0.9778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/200\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.2453 - acc: 0.9619 - val_loss: 0.2474 - val_acc: 0.9556\n",
      "Epoch 122/200\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.2445 - acc: 0.9619 - val_loss: 0.2460 - val_acc: 0.9556\n",
      "Epoch 123/200\n",
      "105/105 [==============================] - 0s 211us/step - loss: 0.2422 - acc: 0.9619 - val_loss: 0.2453 - val_acc: 0.9778\n",
      "Epoch 124/200\n",
      "105/105 [==============================] - 0s 237us/step - loss: 0.2406 - acc: 0.9619 - val_loss: 0.2430 - val_acc: 0.9556\n",
      "Epoch 125/200\n",
      "105/105 [==============================] - 0s 324us/step - loss: 0.2392 - acc: 0.9619 - val_loss: 0.2418 - val_acc: 0.9778\n",
      "Epoch 126/200\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.2376 - acc: 0.9619 - val_loss: 0.2414 - val_acc: 0.9778\n",
      "Epoch 127/200\n",
      "105/105 [==============================] - 0s 250us/step - loss: 0.2378 - acc: 0.9619 - val_loss: 0.2401 - val_acc: 0.9778\n",
      "Epoch 128/200\n",
      "105/105 [==============================] - 0s 208us/step - loss: 0.2340 - acc: 0.9524 - val_loss: 0.2363 - val_acc: 0.9778\n",
      "Epoch 129/200\n",
      "105/105 [==============================] - 0s 233us/step - loss: 0.2347 - acc: 0.9524 - val_loss: 0.2345 - val_acc: 0.9556\n",
      "Epoch 130/200\n",
      "105/105 [==============================] - 0s 226us/step - loss: 0.2322 - acc: 0.9619 - val_loss: 0.2331 - val_acc: 0.9778\n",
      "Epoch 131/200\n",
      "105/105 [==============================] - 0s 254us/step - loss: 0.2313 - acc: 0.9619 - val_loss: 0.2319 - val_acc: 0.9778\n",
      "Epoch 132/200\n",
      "105/105 [==============================] - 0s 290us/step - loss: 0.2295 - acc: 0.9619 - val_loss: 0.2299 - val_acc: 0.9556\n",
      "Epoch 133/200\n",
      "105/105 [==============================] - 0s 275us/step - loss: 0.2275 - acc: 0.9619 - val_loss: 0.2281 - val_acc: 0.9556\n",
      "Epoch 134/200\n",
      "105/105 [==============================] - 0s 266us/step - loss: 0.2271 - acc: 0.9619 - val_loss: 0.2268 - val_acc: 0.9778\n",
      "Epoch 135/200\n",
      "105/105 [==============================] - 0s 254us/step - loss: 0.2265 - acc: 0.9619 - val_loss: 0.2259 - val_acc: 0.9778\n",
      "Epoch 136/200\n",
      "105/105 [==============================] - 0s 316us/step - loss: 0.2248 - acc: 0.9524 - val_loss: 0.2240 - val_acc: 0.9778\n",
      "Epoch 137/200\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.2223 - acc: 0.9619 - val_loss: 0.2231 - val_acc: 0.9778\n",
      "Epoch 138/200\n",
      "105/105 [==============================] - 0s 323us/step - loss: 0.2221 - acc: 0.9619 - val_loss: 0.2218 - val_acc: 0.9778\n",
      "Epoch 139/200\n",
      "105/105 [==============================] - 0s 294us/step - loss: 0.2205 - acc: 0.9524 - val_loss: 0.2190 - val_acc: 0.9778\n",
      "Epoch 140/200\n",
      "105/105 [==============================] - 0s 295us/step - loss: 0.2194 - acc: 0.9524 - val_loss: 0.2175 - val_acc: 0.9778\n",
      "Epoch 141/200\n",
      "105/105 [==============================] - 0s 266us/step - loss: 0.2183 - acc: 0.9524 - val_loss: 0.2160 - val_acc: 0.9556\n",
      "Epoch 142/200\n",
      "105/105 [==============================] - 0s 275us/step - loss: 0.2181 - acc: 0.9619 - val_loss: 0.2149 - val_acc: 0.9778\n",
      "Epoch 143/200\n",
      "105/105 [==============================] - 0s 266us/step - loss: 0.2161 - acc: 0.9619 - val_loss: 0.2139 - val_acc: 0.9778\n",
      "Epoch 144/200\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.2156 - acc: 0.9619 - val_loss: 0.2123 - val_acc: 0.9778\n",
      "Epoch 145/200\n",
      "105/105 [==============================] - 0s 323us/step - loss: 0.2146 - acc: 0.9619 - val_loss: 0.2117 - val_acc: 0.9778\n",
      "Epoch 146/200\n",
      "105/105 [==============================] - 0s 256us/step - loss: 0.2132 - acc: 0.9619 - val_loss: 0.2113 - val_acc: 0.9778\n",
      "Epoch 147/200\n",
      "105/105 [==============================] - 0s 237us/step - loss: 0.2114 - acc: 0.9619 - val_loss: 0.2082 - val_acc: 0.9778\n",
      "Epoch 148/200\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.2098 - acc: 0.9619 - val_loss: 0.2072 - val_acc: 0.9778\n",
      "Epoch 149/200\n",
      "105/105 [==============================] - 0s 198us/step - loss: 0.2116 - acc: 0.9619 - val_loss: 0.2062 - val_acc: 0.9778\n",
      "Epoch 150/200\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.2082 - acc: 0.9619 - val_loss: 0.2060 - val_acc: 0.9778\n",
      "Epoch 151/200\n",
      "105/105 [==============================] - 0s 199us/step - loss: 0.2066 - acc: 0.9619 - val_loss: 0.2033 - val_acc: 0.9778\n",
      "Epoch 152/200\n",
      "105/105 [==============================] - 0s 199us/step - loss: 0.2057 - acc: 0.9619 - val_loss: 0.2025 - val_acc: 0.9778\n",
      "Epoch 153/200\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.2064 - acc: 0.9619 - val_loss: 0.2009 - val_acc: 0.9778\n",
      "Epoch 154/200\n",
      "105/105 [==============================] - 0s 256us/step - loss: 0.2050 - acc: 0.9619 - val_loss: 0.2005 - val_acc: 0.9778\n",
      "Epoch 155/200\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.2047 - acc: 0.9619 - val_loss: 0.1996 - val_acc: 0.9778\n",
      "Epoch 156/200\n",
      "105/105 [==============================] - 0s 285us/step - loss: 0.2016 - acc: 0.9619 - val_loss: 0.1990 - val_acc: 0.9778\n",
      "Epoch 157/200\n",
      "105/105 [==============================] - 0s 266us/step - loss: 0.2024 - acc: 0.9619 - val_loss: 0.1971 - val_acc: 0.9778\n",
      "Epoch 158/200\n",
      "105/105 [==============================] - 0s 256us/step - loss: 0.2011 - acc: 0.9524 - val_loss: 0.1964 - val_acc: 0.9778\n",
      "Epoch 159/200\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.1980 - acc: 0.9619 - val_loss: 0.1940 - val_acc: 0.9778\n",
      "Epoch 160/200\n",
      "105/105 [==============================] - 0s 257us/step - loss: 0.1984 - acc: 0.9619 - val_loss: 0.1938 - val_acc: 0.9778\n",
      "Epoch 161/200\n",
      "105/105 [==============================] - 0s 313us/step - loss: 0.1981 - acc: 0.9619 - val_loss: 0.1928 - val_acc: 0.9778\n",
      "Epoch 162/200\n",
      "105/105 [==============================] - 0s 256us/step - loss: 0.1968 - acc: 0.9619 - val_loss: 0.1918 - val_acc: 0.9778\n",
      "Epoch 163/200\n",
      "105/105 [==============================] - 0s 256us/step - loss: 0.1951 - acc: 0.9619 - val_loss: 0.1888 - val_acc: 0.9778\n",
      "Epoch 164/200\n",
      "105/105 [==============================] - 0s 224us/step - loss: 0.1947 - acc: 0.9619 - val_loss: 0.1882 - val_acc: 0.9778\n",
      "Epoch 165/200\n",
      "105/105 [==============================] - 0s 256us/step - loss: 0.1932 - acc: 0.9619 - val_loss: 0.1889 - val_acc: 0.9778\n",
      "Epoch 166/200\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.1913 - acc: 0.9714 - val_loss: 0.1859 - val_acc: 0.9778\n",
      "Epoch 167/200\n",
      "105/105 [==============================] - 0s 254us/step - loss: 0.1922 - acc: 0.9619 - val_loss: 0.1862 - val_acc: 0.9778\n",
      "Epoch 168/200\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.1920 - acc: 0.9619 - val_loss: 0.1844 - val_acc: 0.9778\n",
      "Epoch 169/200\n",
      "105/105 [==============================] - 0s 284us/step - loss: 0.1899 - acc: 0.9619 - val_loss: 0.1830 - val_acc: 0.9778\n",
      "Epoch 170/200\n",
      "105/105 [==============================] - 0s 304us/step - loss: 0.1887 - acc: 0.9619 - val_loss: 0.1835 - val_acc: 0.9778\n",
      "Epoch 171/200\n",
      "105/105 [==============================] - 0s 237us/step - loss: 0.1900 - acc: 0.9619 - val_loss: 0.1816 - val_acc: 0.9778\n",
      "Epoch 172/200\n",
      "105/105 [==============================] - 0s 218us/step - loss: 0.1863 - acc: 0.9619 - val_loss: 0.1807 - val_acc: 0.9778\n",
      "Epoch 173/200\n",
      "105/105 [==============================] - 0s 313us/step - loss: 0.1867 - acc: 0.9619 - val_loss: 0.1799 - val_acc: 0.9778\n",
      "Epoch 174/200\n",
      "105/105 [==============================] - 0s 323us/step - loss: 0.1860 - acc: 0.9619 - val_loss: 0.1781 - val_acc: 0.9778\n",
      "Epoch 175/200\n",
      "105/105 [==============================] - 0s 256us/step - loss: 0.1849 - acc: 0.9619 - val_loss: 0.1774 - val_acc: 0.9778\n",
      "Epoch 176/200\n",
      "105/105 [==============================] - 0s 237us/step - loss: 0.1846 - acc: 0.9619 - val_loss: 0.1758 - val_acc: 0.9778\n",
      "Epoch 177/200\n",
      "105/105 [==============================] - 0s 294us/step - loss: 0.1824 - acc: 0.9619 - val_loss: 0.1749 - val_acc: 0.9778\n",
      "Epoch 178/200\n",
      "105/105 [==============================] - 0s 255us/step - loss: 0.1842 - acc: 0.9619 - val_loss: 0.1741 - val_acc: 0.9778\n",
      "Epoch 179/200\n",
      "105/105 [==============================] - 0s 236us/step - loss: 0.1829 - acc: 0.9524 - val_loss: 0.1729 - val_acc: 0.9778\n",
      "Epoch 180/200\n",
      "105/105 [==============================] - 0s 218us/step - loss: 0.1807 - acc: 0.9619 - val_loss: 0.1722 - val_acc: 0.9778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/200\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.1793 - acc: 0.9619 - val_loss: 0.1716 - val_acc: 0.9778\n",
      "Epoch 182/200\n",
      "105/105 [==============================] - 0s 218us/step - loss: 0.1782 - acc: 0.9619 - val_loss: 0.1706 - val_acc: 0.9778\n",
      "Epoch 183/200\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.1792 - acc: 0.9619 - val_loss: 0.1699 - val_acc: 0.9778\n",
      "Epoch 184/200\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.1780 - acc: 0.9524 - val_loss: 0.1683 - val_acc: 0.9778\n",
      "Epoch 185/200\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.1761 - acc: 0.9619 - val_loss: 0.1676 - val_acc: 0.9778\n",
      "Epoch 186/200\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.1753 - acc: 0.9619 - val_loss: 0.1665 - val_acc: 0.9778\n",
      "Epoch 187/200\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.1743 - acc: 0.9619 - val_loss: 0.1656 - val_acc: 0.9778\n",
      "Epoch 188/200\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.1734 - acc: 0.9619 - val_loss: 0.1659 - val_acc: 0.9778\n",
      "Epoch 189/200\n",
      "105/105 [==============================] - 0s 218us/step - loss: 0.1736 - acc: 0.9524 - val_loss: 0.1638 - val_acc: 0.9778\n",
      "Epoch 190/200\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.1730 - acc: 0.9619 - val_loss: 0.1635 - val_acc: 0.9778\n",
      "Epoch 191/200\n",
      "105/105 [==============================] - 0s 219us/step - loss: 0.1730 - acc: 0.9619 - val_loss: 0.1623 - val_acc: 0.9778\n",
      "Epoch 192/200\n",
      "105/105 [==============================] - 0s 294us/step - loss: 0.1714 - acc: 0.9619 - val_loss: 0.1609 - val_acc: 0.9778\n",
      "Epoch 193/200\n",
      "105/105 [==============================] - 0s 294us/step - loss: 0.1700 - acc: 0.9619 - val_loss: 0.1601 - val_acc: 0.9778\n",
      "Epoch 194/200\n",
      "105/105 [==============================] - 0s 351us/step - loss: 0.1699 - acc: 0.9619 - val_loss: 0.1599 - val_acc: 0.9778\n",
      "Epoch 195/200\n",
      "105/105 [==============================] - 0s 285us/step - loss: 0.1698 - acc: 0.9619 - val_loss: 0.1585 - val_acc: 0.9778\n",
      "Epoch 196/200\n",
      "105/105 [==============================] - 0s 266us/step - loss: 0.1681 - acc: 0.9619 - val_loss: 0.1578 - val_acc: 0.9778\n",
      "Epoch 197/200\n",
      "105/105 [==============================] - 0s 304us/step - loss: 0.1667 - acc: 0.9619 - val_loss: 0.1569 - val_acc: 0.9778\n",
      "Epoch 198/200\n",
      "105/105 [==============================] - 0s 294us/step - loss: 0.1675 - acc: 0.9619 - val_loss: 0.1559 - val_acc: 0.9778\n",
      "Epoch 199/200\n",
      "105/105 [==============================] - 0s 304us/step - loss: 0.1657 - acc: 0.9619 - val_loss: 0.1555 - val_acc: 0.9778\n",
      "Epoch 200/200\n",
      "105/105 [==============================] - 0s 322us/step - loss: 0.1652 - acc: 0.9619 - val_loss: 0.1544 - val_acc: 0.9778\n"
     ]
    }
   ],
   "source": [
    " history = model.fit(x_train , y_train, validation_data = (x_test ,y_test),epochs=200 , batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 176us/step\n",
      "test_acc: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
